{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here we will be implementing a version of CoCa classifier (the current top performer on ImageNet database) by training it on imagenet imstances of our classes and then asking it to classify our 'strange' images."
      ],
      "metadata": {
        "id": "BjIbDoDC95Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "!pip install -U -q scikeras\n",
        "!pip install -U -q pillow==8.4.0\n",
        "#!pip install -U -q tensorflow\n",
        "##==2.7.0\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "_lIluqNCBJwJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#files.upload(); # upload your kaggle.json file\n",
        "\n",
        "#import json\n",
        "\n",
        "#!mkdir /root/.kaggle/\n",
        "#!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "#!chmod 600 ~/.kaggle/kaggle.json\n",
        "#!kaggle config set -n path -v{/content}"
      ],
      "metadata": {
        "id": "5_6fEW2vBTZg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#be careful, the data is like 160gb +\n",
        "#!kaggle competitions download -c imagenet-object-localization-challenge\n",
        "\n",
        "#CATS AND DOGS\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "#HANDS:\n",
        "downloaded = drive.CreateFile({'id':'1KcMYcNJgtK1zZvfl_9sTqnyBUTri2aP2'}) \n",
        "downloaded.GetContentFile('Hands.zip')"
      ],
      "metadata": {
        "id": "b2r0jnuJAK3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2ad0d4-b18b-423e-8ed5-a7c9dd3096cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-22 11:01:14--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 108.177.126.128, 108.177.127.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  29.1MB/s    in 2.3s    \n",
            "\n",
            "2022-11-22 11:01:16 (29.1 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os \n",
        "import zipfile\n",
        "\n",
        "cd_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(cd_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "h_zip = 'Hands.zip'\n",
        "zip_ref = zipfile.ZipFile(h_zip, 'r')\n",
        "zip_ref.extractall('/tmp/cats_and_dogs_filtered/train')\n",
        "zip_ref.close()\n",
        "\n",
        "#!unzip '/tmp/cats_and_dogs_filtered.zip'\n",
        "#!unzip Hands.zip -d 'cats_and_dogs_filtered/train'\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "tYUt8oBzBrbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4d90f7-4f1c-444c-c760-2f10e8920d74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hands.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up correct directories\n",
        "import numpy as np\n",
        "#from image import PIL\n",
        "\n",
        "#FOR THE MOMENT ALL HANDS AARE IN TRAIN. THIS IS FINE WHATEVER\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "train_hands_dir = os.path.join(train_dir, 'Hands')\n",
        "\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "#NOTE: atm all hands in train dir so no path here\n",
        "\n",
        "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
        "print('total train hand images:', len(os.listdir(train_hands_dir)))\n",
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxOKjunv_zrV",
        "outputId": "ac879a16-4a45-48f2-d3b8-5e680fe4ae07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training cat images: 1000\n",
            "total training dog images: 1000\n",
            "total train hand images: 11076\n",
            "total validation cat images: 500\n",
            "total validation dog images: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PLOT SOME OF THE IMAGES\n",
        "#code followed closely from: https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb#scrollTo=4PIP1rkmeAYS\n",
        "#%matplotlib inline\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#import matplotlib.image as mpimg\n",
        "#import pathlib\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "#nrows = 4\n",
        "#ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "#pic_index = 0\n",
        "#bBaby = io.BytesIo()\n",
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "#fig = plt.gcf()\n",
        "#fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "#pic_index += 8\n",
        "#next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
        "#                for fname in os.listdir(train_cats_dir)[pic_index-8:pic_index]]\n",
        "#next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
        "#                for fname in os.listdir(train_dogs_dir)[pic_index-8:pic_index]]\n",
        "\n",
        "#for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "#  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "#  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "#  img = mpimg.imread(img_path) #EVEN HERE IT ERRORS. CANNOT USE PIL IMAGES FORMAT?????\n",
        "#  plt.imshow(img)\n",
        "\n",
        "#plt.show()\n"
      ],
      "metadata": {
        "id": "DU-C1tgT8jYH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: convert all hand and cat/dog images to same size\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# All images will be rescaled by 1./255 for rgb\n",
        "cdGen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#WAY STATED IN TUTORIAL TO GET ALL FILES IN\n",
        "\n",
        "#batch size=all our images\n",
        "batch_size = len(train_cats_dir) + len(train_dogs_dir) + len(train_hands_dir)\n",
        "\n",
        "train_generator = cdGen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(256, 256),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "num_samples = train_generator.n\n",
        "num_classes = train_generator.num_classes\n",
        "input_shape = train_generator.image_shape\n",
        "\n",
        "classnames = [k for k,v in train_generator.class_indices.items()]\n",
        "\n",
        "X, y = train_generator.next()\n",
        "#X = images\n",
        "#y = classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h1wqqBBOvMR",
        "outputId": "3b70aeeb-bdf6-4255-afc2-7da233064f76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13076 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#clean_directory(train_cats_dir)\n",
        "#clean_directory(train_dogs_dir)\n",
        "#clean_directory(train_hands_dir)\n",
        "\n",
        "#x = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
        "#cnames = os.listdir(train_cats_dir)\n",
        "#dnames = os.listdir(train_dogs_dir)\n",
        "#hnames = os.listdir(train_hands_dir)\n",
        "\n",
        "#Doesn't matter what I do. PIL images do not work\n",
        "#cats_train = [os.path.join(train_cats_dir, f) for f in cnames]\n",
        "#hands_train = [os.path.join(train_hands_dir, f) for f in hnames]\n",
        "#dogs_train = [os.path.join(train_dogs_dir, f) for f in dnames]\n",
        "\n",
        "#This crashed runtime lol but works in theory\n",
        "#hands = np.array([np.array(Image.open(fname)) for fname in hands_train])\n",
        "#cats = np.array([np.array(Image.open(fname)) for fname in cats_train])\n",
        "#dogs = np.array([np.array(Image.open(fname)) for fname in dogs_train])\n"
      ],
      "metadata": {
        "id": "C2iNtE7dh7wY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVkhTRjX91iV",
        "outputId": "b0f21b3a-7f59-4214-93b5-b1f787eb4d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting coca-pytorch\n",
            "  Downloading CoCa_pytorch-0.0.6-py3-none-any.whl (6.3 kB)\n",
            "Collecting einops>=0.4\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 454 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from coca-pytorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->coca-pytorch) (4.1.1)\n",
            "Installing collected packages: einops, coca-pytorch\n",
            "Successfully installed coca-pytorch-0.0.6 einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vit-pytorch\n",
            "  Downloading vit_pytorch-0.38.1-py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (0.13.1+cu113)\n",
            "Requirement already satisfied: einops>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.10->vit-pytorch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->vit-pytorch) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->vit-pytorch) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->vit-pytorch) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit-pytorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit-pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit-pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit-pytorch) (2022.9.24)\n",
            "Installing collected packages: vit-pytorch\n",
            "Successfully installed vit-pytorch-0.38.1\n"
          ]
        }
      ],
      "source": [
        "!pip install coca-pytorch\n",
        "!pip install vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# import vision transformer\n",
        "\n",
        "from vit_pytorch import ViT\n",
        "from vit_pytorch.extractor import Extractor\n",
        "\n",
        "vit = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 1,\n",
        "    num_classes = 3,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 512\n",
        ")\n",
        "\n",
        "vit = Extractor(vit, return_embeddings_only = True, detach = False)\n",
        "\n",
        "# extractor will enable it so the vision transformer returns its embeddings\n",
        "\n",
        "# import CoCa and instantiate it\n",
        "\n",
        "from coca_pytorch.coca_pytorch import CoCa\n",
        "\n",
        "#FIXED: must enable GPU hardware acceleration in runtime settings or this will not work\n",
        "coca = CoCa(\n",
        "    dim = 256,                     # model dimension\n",
        "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
        "    image_dim = 512,               # image embedding dimension, if not the same as model dimensions\n",
        "    num_tokens = 3,                # number of text tokens\n",
        "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
        "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
        "    dim_head = 64,                 # dimension per attention head\n",
        "    heads = 8,                     # number of attention heads\n",
        "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
        "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
        ").cuda()\n",
        "\n",
        "#TODO: need to rescale these basically then we're good\n",
        "text = torch.from_numpy(y).cuda().long()#text = torch.randint(0, 20000, (4, 512)).cuda() .to(device).long()\n",
        "images = torch.from_numpy(X).cuda().long() #images = torch.randn(4, 3, 256, 256).cuda() torch.from_numpy(X)\n",
        "\n",
        "#TODO: we fixed cuda problem, now just need to get input shapes right\n",
        "\n",
        "# train by giving CoCa your text and images with `return_loss = True`\n",
        "\n",
        "loss = coca(\n",
        "    text = text,\n",
        "    images = images,\n",
        "    return_loss = True  # set this to True to get the full caption + contrastive loss\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# TODO ^ train above on imagenet data\n",
        "\n",
        "logits = coca(\n",
        "    text = text,\n",
        "    images = images\n",
        ") # (4, 512, 20000)\n",
        "\n",
        "# and the CLIP-like text and image embeddings as\n",
        "\n",
        "text_embeds, image_embeds = coca(\n",
        "    text = text,\n",
        "    images = images,\n",
        "    return_embeddings = True\n",
        ") # (4, 512), (4, 512)"
      ],
      "metadata": {
        "id": "ORzCeiOt-Vig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "782ff5be-98e0-496b-b2df-3718c4d71b52"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c2ac77af0616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mreturn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# set this to True to get the full caption + contrastive loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/coca_pytorch/coca_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, images, image_tokens, labels, return_loss, return_embeddings)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mtext_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mimage_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# return embeddings if that is what the researcher wants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/coca_pytorch/coca_pytorch.py\u001b[0m in \u001b[0;36membed_image\u001b[0;34m(self, images, image_tokens)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'img_encoder must be passed in for automatic image encoding'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mimage_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# attention pool image tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vit_pytorch/extractor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, return_embeddings_only)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtarget_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vit_pytorch/vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_patch_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [115, 256] but got: [115, 3]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# import vision transformer\n",
        "\n",
        "from vit_pytorch import ViT\n",
        "from vit_pytorch.extractor import Extractor\n",
        "\n",
        "vit = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048\n",
        ")\n",
        "\n",
        "vit = Extractor(vit, return_embeddings_only = True, detach = False)\n",
        "\n",
        "# extractor will enable it so the vision transformer returns its embeddings\n",
        "\n",
        "# import CoCa and instantiate it\n",
        "\n",
        "from coca_pytorch.coca_pytorch import CoCa\n",
        "\n",
        "coca = CoCa(\n",
        "    dim = 512,                     # model dimension\n",
        "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
        "    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
        "    num_tokens = 20000,            # number of text tokens\n",
        "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
        "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
        "    dim_head = 64,                 # dimension per attention head\n",
        "    heads = 8,                     # number of attention heads\n",
        "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
        "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
        ").cuda()\n",
        "\n",
        "# mock text and images\n",
        "\n",
        "text = torch.randint(0, 20000, (4, 512)).cuda()\n",
        "#text is 512 long by 4 rows\n",
        "images = torch.randn(4, 3, 256, 256).cuda()\n",
        "#images is 4 -> long by 3 rows, each of 256x256 dimensions\n",
        "\n",
        "#think to matrix multiplication: 3x4 X 4x512 = 3x512\n",
        "print(images)\n",
        "\n",
        "# train by giving CoCa your text and images with `return_loss = True`\n",
        "\n",
        "loss = coca(\n",
        "    text = text,\n",
        "    images = images,\n",
        "    return_loss = True  # set this to True to get the full caption + contrastive loss\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# do the above for as much text and images...\n",
        "# then you can get the caption logits as so\n",
        "\n",
        "logits = coca(\n",
        "    text = text,\n",
        "    images = images\n",
        ") # (4, 512, 20000)\n",
        "\n",
        "# and the CLIP-like text and image embeddings as\n",
        "\n",
        "text_embeds, image_embeds = coca(\n",
        "    text = text,\n",
        "    images = images,\n",
        "    return_embeddings = True\n",
        ") # (4, 512), (4, 512)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1OHh-nOC7dT",
        "outputId": "ad3fdc56-6f9e-4109-8a14-eb6f38edf9aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-3.3463e-01, -1.3828e+00,  1.6301e-01,  ..., -6.8082e-01,\n",
            "            1.0328e+00, -1.7068e+00],\n",
            "          [ 2.6002e-01, -3.0462e-01, -1.8600e+00,  ..., -4.6944e-01,\n",
            "           -4.0507e-01,  3.7863e-01],\n",
            "          [ 6.8310e-02,  2.2696e-01,  5.0837e-01,  ...,  1.6101e+00,\n",
            "            5.1867e-02, -5.9583e-01],\n",
            "          ...,\n",
            "          [ 1.7805e-01,  3.9538e-01,  1.0447e+00,  ...,  1.7420e+00,\n",
            "           -9.1518e-01, -4.8141e-01],\n",
            "          [ 7.5213e-01,  5.4313e-03,  8.2938e-01,  ..., -1.1825e+00,\n",
            "           -4.4107e-01,  4.4723e-02],\n",
            "          [-5.2621e-01, -1.9041e-01,  5.4493e-01,  ..., -5.1355e-01,\n",
            "           -3.1809e-01,  8.6003e-01]],\n",
            "\n",
            "         [[ 1.1269e+00, -1.1397e+00, -6.5762e-01,  ...,  1.4160e+00,\n",
            "            1.9983e+00, -1.1635e+00],\n",
            "          [-1.1897e-01, -1.4436e+00,  1.2104e+00,  ..., -1.5964e+00,\n",
            "            4.2903e-01, -1.1325e+00],\n",
            "          [-3.2489e-01,  1.3638e-01,  2.0786e-01,  ...,  7.4081e-01,\n",
            "           -1.0589e+00,  1.7272e-01],\n",
            "          ...,\n",
            "          [-1.1809e+00,  3.5983e-02,  1.8662e-01,  ..., -4.0949e-01,\n",
            "           -4.4289e-01, -5.2986e-01],\n",
            "          [ 1.1057e-01,  5.3205e-01, -2.8289e-01,  ...,  1.1753e+00,\n",
            "            9.6098e-01, -1.1391e+00],\n",
            "          [-1.7324e-01,  1.2381e+00,  2.8573e-01,  ..., -5.2746e-01,\n",
            "           -3.0762e-01,  1.0249e+00]],\n",
            "\n",
            "         [[ 2.3247e-01,  8.4317e-01, -2.6184e+00,  ..., -1.4965e-01,\n",
            "            1.3828e+00, -3.7609e-02],\n",
            "          [-1.2013e+00,  1.4515e+00,  1.1645e+00,  ...,  6.4748e-01,\n",
            "           -2.3757e-01, -8.3239e-01],\n",
            "          [ 9.8534e-01,  7.8999e-01, -1.3817e+00,  ...,  1.0614e+00,\n",
            "           -3.2462e-01, -3.1593e+00],\n",
            "          ...,\n",
            "          [-1.0251e+00, -1.3875e+00,  2.0305e-01,  ..., -9.1142e-01,\n",
            "           -7.9668e-01, -2.5862e+00],\n",
            "          [ 6.7233e-01,  4.6622e-01, -2.3103e+00,  ..., -3.1308e-01,\n",
            "           -4.4597e-02,  9.5173e-01],\n",
            "          [ 4.0255e-01,  9.0545e-01, -6.9079e-01,  ...,  1.4931e+00,\n",
            "           -8.2757e-01,  9.6690e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.9635e-01,  8.1940e-01,  1.1372e+00,  ..., -9.3446e-01,\n",
            "           -1.7769e+00,  1.5438e-01],\n",
            "          [ 9.1623e-01,  7.9284e-01,  5.8993e-01,  ..., -1.4339e+00,\n",
            "           -1.3112e+00,  4.5801e-01],\n",
            "          [-1.6591e-01, -1.0762e+00,  9.6132e-02,  ...,  3.2532e-01,\n",
            "            1.8011e-01, -3.8987e-01],\n",
            "          ...,\n",
            "          [-1.5895e+00, -6.4751e-01,  4.2497e-01,  ...,  7.0181e-01,\n",
            "            9.3987e-01, -4.7444e-01],\n",
            "          [ 1.5548e+00,  1.2662e+00, -7.3868e-01,  ..., -3.8757e-01,\n",
            "           -1.0852e+00, -2.9255e-01],\n",
            "          [-3.3431e-02, -1.9342e+00,  1.6373e+00,  ...,  1.3838e+00,\n",
            "            2.6051e+00,  8.8377e-01]],\n",
            "\n",
            "         [[ 4.7982e-01,  1.0174e+00,  1.7779e+00,  ...,  7.4348e-01,\n",
            "            9.9605e-01, -2.2457e+00],\n",
            "          [ 1.1706e+00,  5.4929e-01,  2.9498e-01,  ...,  9.2221e-01,\n",
            "           -2.0976e-01,  8.5955e-01],\n",
            "          [ 9.1075e-01, -6.7796e-01, -7.8353e-01,  ..., -1.2633e+00,\n",
            "            1.1439e+00,  1.0296e+00],\n",
            "          ...,\n",
            "          [ 9.9301e-01,  8.6332e-01, -1.6656e-01,  ...,  6.5274e-01,\n",
            "            6.1683e-01, -3.8944e-01],\n",
            "          [-1.6364e+00, -9.6978e-01, -2.6347e-01,  ..., -1.8853e-01,\n",
            "           -9.1444e-01, -1.0049e+00],\n",
            "          [ 5.4655e-01,  7.2906e-03,  1.6503e-01,  ..., -1.1550e+00,\n",
            "            6.4390e-02,  2.0833e-01]],\n",
            "\n",
            "         [[ 7.0623e-01,  9.5098e-01,  1.7040e-01,  ...,  6.6720e-01,\n",
            "           -1.8146e-01, -1.4772e+00],\n",
            "          [-2.9896e-01,  1.1669e+00, -1.7054e+00,  ..., -1.8038e-01,\n",
            "           -3.9302e-01, -1.4190e+00],\n",
            "          [-7.9453e-01, -7.7096e-01, -2.3618e+00,  ..., -8.9194e-02,\n",
            "           -1.8720e+00,  1.0587e+00],\n",
            "          ...,\n",
            "          [ 4.7768e-01,  1.8506e+00, -4.3102e-01,  ...,  2.7458e-01,\n",
            "            3.7130e-03,  8.0532e-01],\n",
            "          [-5.4696e-01, -4.3818e-01, -8.6079e-01,  ...,  7.9265e-01,\n",
            "           -1.9450e+00, -8.2985e-02],\n",
            "          [-1.5418e+00,  1.5121e+00,  4.3068e-01,  ..., -8.5446e-01,\n",
            "            3.2142e-01, -2.4732e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1946e+00, -4.1085e-01,  3.9258e-01,  ...,  1.0754e+00,\n",
            "            6.1940e-01,  4.3606e-01],\n",
            "          [ 6.3666e-01,  4.3341e-01, -2.0892e+00,  ..., -6.1947e-01,\n",
            "            8.4844e-01,  4.4198e-01],\n",
            "          [ 4.2100e-01, -3.1640e-01, -1.1069e+00,  ..., -6.5961e-01,\n",
            "           -2.2072e+00,  5.3197e-02],\n",
            "          ...,\n",
            "          [-1.9634e-01, -1.3412e+00, -2.8483e-01,  ..., -5.3197e-01,\n",
            "            9.5411e-02, -1.5399e+00],\n",
            "          [-7.6404e-01,  4.5703e-01,  1.7204e-01,  ..., -1.2272e-01,\n",
            "            1.0985e+00, -1.7546e-01],\n",
            "          [ 9.0558e-02,  8.0138e-01,  3.0209e-01,  ...,  3.9013e-01,\n",
            "           -3.7994e-01,  1.3362e-01]],\n",
            "\n",
            "         [[ 7.1639e-01,  1.1432e+00,  2.8173e-01,  ..., -7.1867e-01,\n",
            "           -3.9512e-01, -1.1811e+00],\n",
            "          [ 9.4805e-01, -2.6894e+00,  1.0951e+00,  ...,  4.2535e-01,\n",
            "            1.5117e+00,  7.5192e-01],\n",
            "          [ 8.4544e-01,  3.7119e-01,  8.1494e-02,  ...,  1.1113e+00,\n",
            "           -1.8846e+00, -1.0229e+00],\n",
            "          ...,\n",
            "          [-6.1408e-01, -6.5511e-01, -2.0278e+00,  ...,  2.0726e-01,\n",
            "            5.0327e-01, -1.5133e+00],\n",
            "          [ 2.1845e+00, -5.4803e-01, -1.8744e+00,  ..., -6.1681e-01,\n",
            "            4.2650e-01, -7.0238e-01],\n",
            "          [ 8.0905e-02, -2.1004e-01, -6.6806e-02,  ...,  2.0807e+00,\n",
            "           -1.8530e-01, -1.6010e-01]],\n",
            "\n",
            "         [[-7.6508e-01,  2.4209e+00,  2.2546e+00,  ..., -1.2727e+00,\n",
            "           -1.8474e+00,  7.7513e-01],\n",
            "          [ 4.0426e-01,  6.8186e-02,  6.7911e-01,  ...,  4.2542e-01,\n",
            "            3.0310e+00, -1.0622e+00],\n",
            "          [-3.8068e-01, -1.2367e+00, -1.1479e+00,  ..., -1.6132e+00,\n",
            "            1.0996e+00, -1.9625e-01],\n",
            "          ...,\n",
            "          [-1.0252e+00, -8.1223e-01, -7.2725e-01,  ..., -1.5328e-01,\n",
            "           -1.0615e+00,  3.2252e-01],\n",
            "          [ 6.2912e-01,  1.6770e+00,  5.9600e-01,  ..., -1.2127e-01,\n",
            "           -1.1325e-01,  1.8258e+00],\n",
            "          [-1.9860e-02, -2.2237e+00, -1.7314e-01,  ..., -4.5114e-01,\n",
            "            7.2762e-01,  1.0650e+00]]],\n",
            "\n",
            "\n",
            "        [[[-1.0216e-01, -2.2285e+00, -7.5987e-01,  ..., -1.9554e-03,\n",
            "            6.8125e-01, -2.0047e+00],\n",
            "          [ 1.6469e-01,  7.9096e-02,  7.7317e-01,  ..., -2.7659e-01,\n",
            "            1.0272e+00,  5.3815e-01],\n",
            "          [ 5.4208e-01,  4.7967e-01,  6.0176e-01,  ...,  6.9662e-01,\n",
            "           -9.1061e-01, -6.0648e-01],\n",
            "          ...,\n",
            "          [-1.6819e+00,  6.2914e-01,  8.1818e-01,  ..., -7.6943e-01,\n",
            "            9.1105e-01, -3.2857e-01],\n",
            "          [-2.2413e-01,  9.5241e-02,  1.1946e+00,  ..., -1.6254e+00,\n",
            "            2.5126e-01, -9.3543e-01],\n",
            "          [-8.5002e-01, -3.3838e-01, -4.2724e-01,  ..., -1.5093e+00,\n",
            "           -7.8925e-01, -6.6798e-01]],\n",
            "\n",
            "         [[-1.4026e+00, -1.5286e+00,  4.3232e-01,  ..., -7.9439e-01,\n",
            "            4.5398e-01,  8.5397e-01],\n",
            "          [ 9.9714e-01, -1.4207e-01,  3.1606e-01,  ...,  5.0462e-01,\n",
            "            4.4818e-01,  1.7508e-01],\n",
            "          [-3.6676e-01,  5.2485e-01,  5.4424e-01,  ..., -8.9561e-02,\n",
            "            1.4586e-01, -4.0097e-01],\n",
            "          ...,\n",
            "          [-4.2223e-01,  1.5725e+00,  6.9085e-01,  ..., -1.0405e+00,\n",
            "           -9.8769e-01, -5.2586e-01],\n",
            "          [-1.9153e+00, -7.5953e-02, -2.8836e-01,  ..., -6.5471e-02,\n",
            "            3.7810e-01, -1.4720e-01],\n",
            "          [ 1.1884e+00,  6.3557e-01,  8.9922e-02,  ..., -1.6820e-01,\n",
            "            2.7583e-03,  9.7894e-01]],\n",
            "\n",
            "         [[-1.0593e+00,  2.6865e-01, -7.6187e-01,  ..., -1.0416e+00,\n",
            "           -1.1685e+00,  8.3696e-01],\n",
            "          [-6.0959e-01,  2.6796e-01, -1.2052e+00,  ...,  2.0527e+00,\n",
            "            1.0997e+00,  1.9363e+00],\n",
            "          [ 1.3937e+00, -1.1720e+00,  1.6515e+00,  ..., -8.6108e-01,\n",
            "            7.9622e-01, -3.6307e-01],\n",
            "          ...,\n",
            "          [ 5.6602e-01, -3.9613e-01,  4.6107e-01,  ..., -4.0168e-01,\n",
            "            4.4059e-01,  4.7573e-01],\n",
            "          [-1.8180e+00,  1.8749e+00, -3.5220e-01,  ..., -5.9433e-01,\n",
            "            8.9327e-01, -2.1076e+00],\n",
            "          [-8.8658e-01, -8.1081e-01, -2.1262e-01,  ..., -9.8819e-01,\n",
            "           -2.4449e-01, -2.2959e+00]]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CITATIONS/REFERENCES:\n",
        "\n",
        "@article{afifi201911kHands, \n",
        "title = {11K Hands: gender recognition and biometric identification using a large dataset of hand images}, author = {Afifi, Mahmoud}, journal = {Multimedia Tools and Applications}, doi = {10.1007/s11042-019-7424-8}, url = {https://doi.org/10.1007/s11042-019-7424-8}, year={2019} \n",
        "\n",
        "@inproceedings{Yu2022CoCaCC,\n",
        "  title   = {CoCa: Contrastive Captioners are Image-Text Foundation Models},\n",
        "  author  = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},\n",
        "  year    = {2022}\n",
        "}}\n",
        "\n",
        "@Lots of code referenced from:\n",
        "https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb#scrollTo=TkNQqjZaB4_u"
      ],
      "metadata": {
        "id": "GficiptWEkGH"
      }
    }
  ]
}