{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here we will be implementing a version of CoCa classifier (the current top performer on ImageNet database) by training it on imagenet imstances of our classes and then asking it to classify our 'strange' images."
      ],
      "metadata": {
        "id": "BjIbDoDC95Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "!pip install -U -q scikeras\n",
        "!pip install -U -q Pillow\n",
        "!pip install -U -q tensorflow\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "_lIluqNCBJwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3788d548-793a-4d3f-e24c-980c94a0f49a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.2 MB 11.4 MB/s \n",
            "\u001b[?25h\u001b[31mERROR: Invalid requirement: 'tensorflow#==2.7.0'\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (9.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#files.upload(); # upload your kaggle.json file\n",
        "\n",
        "#import json\n",
        "\n",
        "#!mkdir /root/.kaggle/\n",
        "#!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "#!chmod 600 ~/.kaggle/kaggle.json\n",
        "#!kaggle config set -n path -v{/content}"
      ],
      "metadata": {
        "id": "5_6fEW2vBTZg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#be careful, the data is like 160gb +\n",
        "#!kaggle competitions download -c imagenet-object-localization-challenge\n",
        "\n",
        "#CATS AND DOGS\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "#HANDS:\n",
        "downloaded = drive.CreateFile({'id':'1KcMYcNJgtK1zZvfl_9sTqnyBUTri2aP2'}) \n",
        "downloaded.GetContentFile('Hands.zip')"
      ],
      "metadata": {
        "id": "b2r0jnuJAK3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d6c0c5-fc69-4ea3-f3d8-5f4dd4fd95da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-21 11:25:05--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.143.128, 173.194.69.128, 173.194.79.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.143.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  25.4MB/s    in 2.6s    \n",
            "\n",
            "2022-11-21 11:25:07 (25.4 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import zipfile\n",
        "#TODO: how i unzip is the only way I can think of this may not be working\n",
        "\n",
        "cd_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(cd_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "h_zip = 'Hands.zip'\n",
        "zip_ref = zipfile.ZipFile(h_zip, 'r')\n",
        "zip_ref.extractall('/tmp/cats_and_dogs_filtered/train')\n",
        "zip_ref.close()\n",
        "#!unzip '/tmp/cats_and_dogs_filtered.zip'\n",
        "#if os.path.exists('cats_and_dogs_filtered/train/hands') == False:\n",
        "#  os.mkdir('cats_and_dogs_filtered/train/hands')\n",
        "#!unzip Hands.zip -d 'cats_and_dogs_filtered/train'\n",
        "#TODO: reduce sie of hands, take maybe 1500 images or smn\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "tYUt8oBzBrbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1098fb0a-43d1-4db9-a866-b628eab1d3e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hands.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: wrap all of these in IO.byte stream fucking whatever suck fuck "
      ],
      "metadata": {
        "id": "Jpyzl5RKzT9W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up correct directories\n",
        "import numpy as np\n",
        "#from image import PIL\n",
        "\n",
        "#FOR THE MOMENT ALL HANDS AARE IN TRAIN. THIS IS FINE WHATEVER\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "train_hands_dir = os.path.join(train_dir, 'Hands')\n",
        "\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "#NOTE: atm all hands in train dir so no path here\n",
        "\n",
        "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
        "print('total train hand images:', len(os.listdir(train_hands_dir)))\n",
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxOKjunv_zrV",
        "outputId": "55d5a41a-8b8c-4c72-9a15-71a1adb35d44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training cat images: 1000\n",
            "total training dog images: 1000\n",
            "total train hand images: 11077\n",
            "total validation cat images: 500\n",
            "total validation dog images: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0\n",
        "\n",
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
        "                for fname in os.listdir(train_cats_dir)[pic_index-8:pic_index]]\n",
        "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
        "                for fname in os.listdir(train_dogs_dir)[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path) #EVEN HERE IT ERRORS. CANNOT USE PIL IMAGES FORMAT?????\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "DU-C1tgT8jYH",
        "outputId": "c807ebe8-4317-4459-8dce-9c34956f31e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2738542ce3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Off'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Don't show axes (or gridlines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             raise ValueError('Only know how to handle PNG; with Pillow '\n\u001b[1;32m   1463\u001b[0m                              'installed, Matplotlib can handle more images')\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mdecodes\u001b[0m \u001b[0mpixel\u001b[0m \u001b[0mdata\u001b[0m \u001b[0monly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2896\u001b[0;31m     \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/tmp/cats_and_dogs_filtered/train/cats/cat.287.jpg'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANAAAADLCAYAAAAftR0sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACNUlEQVR4nO3TwQ3AIBDAsNL9dz52IA+EZE+QT9bMfMCZ/3YAvMxAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEBgIAgNBYCAIDASBgSAwEAQGgsBAEGzOpQSTSImA5gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from glob import glob\n",
        "#import os\n",
        "\n",
        "#def clean_directory(dir_path, ext=\".jpg\"):\n",
        "#    files = glob(os.path.join(dir_path, \".*\" + ext))  # this line find all files witch starts with . and ends with given extension\n",
        "#    for file_path in files:\n",
        "#        os.remove(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "2aZq6RMDsJgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: convert all hand and cat/dog images to same size\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "\n",
        "# All images will be rescaled by 1./255 for rgb\n",
        "cdGen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#WAY STATED IN TUTORIAL TO GET ALL FILES IN\n",
        "\n",
        "#batch size=all our images\n",
        "batch_size = len(train_cats_dir) + len(train_dogs_dir) + len(train_hands_dir)\n",
        "\n",
        "train_generator = cdGen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(256, 256),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "num_samples = train_generator.n\n",
        "num_classes = train_generator.num_classes\n",
        "input_shape = train_generator.image_shape\n",
        "\n",
        "classnames = [k for k,v in train_generator.class_indices.items()]\n",
        "\n",
        "X, y = train_generator.next()\n",
        "print(X)\n",
        "#print(y)\n",
        "#NOTE: NICE. WE SHOULD HAVE THEM ALL RESIZED)\n",
        "\n",
        "#TODO: THIS IS A PIPELINE ^\n",
        "#->HAVE TO ACTUALLY EMPLOY IT TO GET THE IMAGES TRANSFORMED, AND STORE EM SOMEWHERE\n",
        "#IN NP ARRAYS \n",
        "\n",
        "#NEED TO ORGANISE SO ITS LIKE:\n",
        "#->MAIN DIR\n",
        "#   -> CLASS A\n",
        "#       ->IMAGES\n",
        "#   -> CLASS B\n",
        "#       ->IMAGES\n",
        "#   ->CLASS C\n",
        "#       ->IMAGES\n",
        "\n",
        "#dogs/cats are 500,375\n",
        "#HANDS are 1600x1200\n",
        "\n",
        "#ATM these are arrays of directories, can do a loop below to laod all if we want\n",
        "\n",
        "#img = load_img([PASS ARRAY AND INDEX], target_size=(150, 150))  # this is a PIL image\n",
        "#x = img_to_array(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "-h1wqqBBOvMR",
        "outputId": "d3f428f3-ee08-4412-9bc9-485a5bf2a65f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19076 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-14f579f63ac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mclassnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#print(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m           keep_aspect_ratio=self.keep_aspect_ratio)\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m       \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    392\u001b[0m       \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     raise TypeError('path should be path-like or io.BytesIO'\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mdecodes\u001b[0m \u001b[0mpixel\u001b[0m \u001b[0mdata\u001b[0m \u001b[0monly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2896\u001b[0;31m     \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7fe47d4c6770>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#clean_directory(train_cats_dir)\n",
        "#clean_directory(train_dogs_dir)\n",
        "#clean_directory(train_hands_dir)\n",
        "\n",
        "#x = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
        "cnames = os.listdir(train_cats_dir)\n",
        "dnames = os.listdir(train_dogs_dir)\n",
        "hnames = os.listdir(train_hands_dir)\n",
        "\n",
        "#Doesn't matter what I do. PIL images do not work\n",
        "cats_train = [os.path.join(train_cats_dir, f) for f in cnames]\n",
        "hands_train = [os.path.join(train_hands_dir, f) for f in hnames]\n",
        "dogs_train = [os.path.join(train_dogs_dir, f) for f in dnames]\n",
        "\n",
        "#LODING IN ARRAYS IN WAY STATED IN TUTORIAL, EVEN THOUGH IT DOESNT WORK\n",
        "#for p in cats_train:\n",
        "#  img = load_img(p, target_size=(256, 256))  # this is a PIL image\n",
        "#  x = img_to_array(img)\n",
        "\n",
        "\n",
        "#ERROR FOR ALL IMAGES\n",
        "#for img_p in cnames:\n",
        "#    try:\n",
        "#        img = PIL.Image.open(train_cats_dir + \"/\" + img_p)\n",
        "#    except PIL.UnidentifiedImageError:\n",
        "#            print(img_p)\n",
        "\n",
        "\n",
        "#USE THE ARRAYS ABOVE TO OPEN ALL INTO THEIR FRAMES\n",
        "#hands = np.array([np.array(Image.open(io.BytesIO(fname))) for fname in hands_train])\n",
        "#cats = np.array([np.array(Image.open(io.BytesIO(fname))) for fname in cats_train])\n",
        "#dogs = np.array([np.array(Image.open(io.BytesIO(fname))) for fname in dogs_train])\n"
      ],
      "metadata": {
        "id": "C2iNtE7dh7wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "#cnames = os.listdir(train_cats_dir)\n",
        "#cvnames = os.listdir(validation_cats_dir)\n",
        "#dnames = os.listdir(train_dogs_dir)\n",
        "#dvnames = os.listdir(validation_dogs_dir)\n",
        "#hnames = os.listdir(train_hands_dir)\n",
        "\n",
        "#cats_train = [os.path.join(train_cats_dir, f) for f in cnames]\n",
        "#cats_val = [os.path.join(validation_cats_dir, f) for f in cvnames]\n",
        "#dogs_train = [os.path.join(train_dogs_dir, f) for f in dnames]\n",
        "#dogs_val = [os.path.join(validation_dogs_dir, f) for f in dvnames]\n",
        "#hands_train = [os.path.join(train_hands_dir, f) for f in hnames]\n",
        "\n",
        "#hands_train = hands[:1000]\n",
        "#hands_val = hands[1000:1500]\n",
        "\n",
        "#df_train = {}\n",
        "#df_val = {}\n",
        "\n",
        "#df_train['image'] = cats_train\n",
        "#df_train['class'] = ([0]* len(cats_train))\n",
        "#df_train['image'].append(dogs_train)\n",
        "#df_train['class'].append([1]*len(dogs_train))\n",
        "#df_train['image'].append(hands_train)\n",
        "#df_train['class'].append([2]*len(hands_train))\n",
        "\n",
        "#df_val['image'] = cats_val\n",
        "#df_val['class'] = ([0]* len(cats_val))\n",
        "#df_val['image'].append(dogs_val)\n",
        "#df_val['class'].append([1]*len(dogs_val))\n",
        "#df_val['image'].append(hands_val)\n",
        "#df_val['class'].append([2]*len(hands_val))"
      ],
      "metadata": {
        "id": "LIv0U_MxMBJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVkhTRjX91iV"
      },
      "outputs": [],
      "source": [
        "!pip install coca-pytorch\n",
        "!pip install vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Image preprocessing\n",
        "#import io\n",
        "\n",
        "#import torchvision.transforms as T\n",
        "\n",
        "#from PIL import Image\n",
        "#resp = #IMAGEFILE\n",
        "#img = Image.open(io.BytesIO(resp.content))\n",
        "\n",
        "#preprocess = T.Compose([\n",
        "#   T.Resize(256),\n",
        "#   T.CenterCrop(224),\n",
        "#   T.ToTensor(),\n",
        "#   T.Normalize(\n",
        "#       mean=[0.485, 0.456, 0.406],\n",
        "#       std=[0.229, 0.224, 0.225]\n",
        "#   )\n",
        "#])\n",
        "\n",
        "#x = preprocess(img)\n",
        "#x.shape\n",
        "#THIS SHOULD BE ABLE TO CONVERT ANY IMAGE INDIVIDUALLY ^, JUST LOOP FOR ALL OR WHATEVER\n",
        "#AND, DECIDE WHAT SHAPE IMAGES SHOULD BE ETC, RESIZE MAIN THING\n",
        "\n",
        "#dog = \n",
        "#cat = \n",
        "#hands = "
      ],
      "metadata": {
        "id": "2v671DKd8ACu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# import vision transformer\n",
        "\n",
        "from vit_pytorch import ViT\n",
        "from vit_pytorch.extractor import Extractor\n",
        "\n",
        "vit = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048\n",
        ")\n",
        "\n",
        "#TODO: try and get this to work for just cats and dogs\n",
        "#TODO: pretty sure labels are needed too\n",
        "vit = Extractor(vit, return_embeddings_only = True, detach = False)\n",
        "\n",
        "# extractor will enable it so the vision transformer returns its embeddings\n",
        "\n",
        "# import CoCa and instantiate it\n",
        "\n",
        "from coca_pytorch.coca_pytorch import CoCa\n",
        "\n",
        "#FIXED: must enable GPU hardware acceleration in runtime settings or this will not work\n",
        "coca = CoCa(\n",
        "    dim = 512,                     # model dimension\n",
        "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
        "    #image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
        "    num_tokens = 3,            # number of text tokens\n",
        "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
        "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
        "    dim_head = 64,                 # dimension per attention head\n",
        "    heads = 8,                     # number of attention heads\n",
        "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
        "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
        ").cuda()\n",
        "\n",
        "#TODO: replace these with imageNet instances\n",
        "\n",
        "text = torch.from_numpy(train_generator.classes) #.cuda()\n",
        "images = torch.from_numpy(train_generator) #.cuda()\n",
        "\n",
        "# train by giving CoCa your text and images with `return_loss = True`\n",
        "\n",
        "loss = coca(\n",
        "    text = text,\n",
        "    images = images\n",
        "    #return_loss = True  # set this to True to get the full caption + contrastive loss\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# TODO ^ train above on imagenet data\n",
        "\n",
        "logits = coca(\n",
        "    text = text,\n",
        "    images = images\n",
        ") # (4, 512, 20000)\n",
        "\n",
        "# and the CLIP-like text and image embeddings as\n",
        "\n",
        "text_embeds, image_embeds = coca(\n",
        "    text = text,\n",
        "    images = images,\n",
        "    return_embeddings = True\n",
        ") # (4, 512), (4, 512)"
      ],
      "metadata": {
        "id": "ORzCeiOt-Vig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CITATIONS/REFERENCES:\n",
        "\n",
        "@article{afifi201911kHands, \n",
        "title = {11K Hands: gender recognition and biometric identification using a large dataset of hand images}, author = {Afifi, Mahmoud}, journal = {Multimedia Tools and Applications}, doi = {10.1007/s11042-019-7424-8}, url = {https://doi.org/10.1007/s11042-019-7424-8}, year={2019} \n",
        "\n",
        "@inproceedings{Yu2022CoCaCC,\n",
        "  title   = {CoCa: Contrastive Captioners are Image-Text Foundation Models},\n",
        "  author  = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},\n",
        "  year    = {2022}\n",
        "}}\n",
        "\n",
        "@Lots of code referenced from:\n",
        "https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb#scrollTo=TkNQqjZaB4_u"
      ],
      "metadata": {
        "id": "GficiptWEkGH"
      }
    }
  ]
}